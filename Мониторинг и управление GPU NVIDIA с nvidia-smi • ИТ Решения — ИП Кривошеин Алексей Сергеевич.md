---
page-title: "Мониторинг и управление GPU NVIDIA с nvidia-smi • ИТ Решения — ИП Кривошеин Алексей Сергеевич"
url: https://krivoshein.site/monitoring-i-upravlenie-gpu-nvidia-s-nvidia-smi/
date: "2025-02-25 09:23:03"
---

> watch -n 2 nvidia-smi

---

## Введение

Когда речь заходит о мониторинге и управлении видеокартами NVIDIA, первое, что приходит на ум инженерам и системным администраторам, — это nvidia-smi. Эта утилита командной строки позволяет не только наблюдать за состоянием графических процессоров, но и ограничивать энергопотребление, логировать данные и анализировать загрузку GPU в реальном времени.

В этой статье мы подробно разберем основные команды **nvidia-smi**, их применение и практические сценарии использования.

---

## 1\. Базовая информация о GPU

Чтобы получить сводку о состоянии всех установленных видеокарт, достаточно выполнить:

nvidia-smi

Вывод команды включает:

-   **Загрузку GPU (%)**
-   **Температуру (°C)**
-   **Потребление энергии (Вт)**
-   **Объем занятой видеопамяти (MB)**
-   **Активные процессы, использующие GPU**

Пример вывода:

+-----------------------------------------------------------------------------+

| NVIDIA-SMI 535.104.05 Driver Version: 535.104.05 CUDA Version: 12.2 |

|-------------------------------+----------------------+----------------------+

| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |

| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |

| 0 NVIDIA RTX 4090 On | 00000000:01:00.0 Off | Off |

| 34% 65C P8 15W / 450W | 4000MiB / 24576MiB | 12% Default |

+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+ | NVIDIA-SMI 535.104.05 Driver Version: 535.104.05 CUDA Version: 12.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | | | 0 NVIDIA RTX 4090 On | 00000000:01:00.0 Off | Off | | 34% 65C P8 15W / 450W | 4000MiB / 24576MiB | 12% Default | +-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05              Driver Version: 535.104.05 CUDA Version: 12.2 |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |                          |
|  0   NVIDIA RTX 4090    On     | 00000000:01:00.0 Off |                  Off |
|  34%   65C    P8     15W / 450W |    4000MiB / 24576MiB |      12%     Default |
+-------------------------------+----------------------+----------------------+

Для обновления информации в реальном времени можно использовать **watch**:

watch -n 2 nvidia-smi

Каждые 2 секунды на экран будет выводиться актуальная информация.

---

## 2\. Просмотр версии драйвера и CUDA

Если необходимо быстро узнать, какая версия драйвера и **CUDA** установлена, можно выполнить:

nvidia-smi --query-gpu=driver\_version,cuda\_version --format=csv

nvidia-smi --query-gpu=driver\_version,cuda\_version --format=csv

nvidia-smi --query-gpu=driver\_version,cuda\_version --format=csv

Вывод будет следующим:

driver\_version, cuda\_version

driver\_version, cuda\_version 535.104.05, 12.2

driver\_version, cuda\_version
535.104.05, 12.2

---

## 3\. Контроль энергопотребления

Видеокарты NVIDIA могут потреблять значительное количество энергии, что особенно критично для серверов и рабочих станций. Для мониторинга энергопотребления можно выполнить:

nvidia-smi --query-gpu=power.draw --format=csv

nvidia-smi --query-gpu=power.draw --format=csv

nvidia-smi --query-gpu=power.draw --format=csv

Вывод покажет текущее энергопотребление, например:

power.draw \[W\]
125.46 W

Если требуется **ограничить энергопотребление**, например, до 100 Вт, используется команда:

nvidia-smi -pl 100

Чтобы проверить, применилось ли ограничение, повторно выполните команду мониторинга.

---

## 4\. Отслеживание процессов, использующих GPU

Бывает необходимо узнать, какие именно процессы нагружают видеокарту. Для этого существует команда:

nvidia-smi pmon -s um

Вывод команды покажет список **PID процессов**, объем потребляемой видеопамяти и загрузку GPU для каждого из них.

Пример:

\# gpu pid type sm mem enc dec command

0 1245 C 50% 2048 0 0 python3

\# gpu pid type sm mem enc dec command 0 1245 C 50% 2048 0 0 python3 0 3765 G 5% 512 0 0 Xorg

\# gpu    pid   type    sm   mem   enc   dec    command
0       1245    C     50%  2048     0     0    python3
0       3765    G      5%   512     0     0    Xorg

-   **C** — процесс CUDA
-   **G** — графический процесс

Если замечена аномальная загрузка, можно принудительно завершить процесс:

kill -9 1245

---

## 5\. Логирование данных

Если нужно вести мониторинг в течение длительного времени и записывать данные в файл, можно использовать логирование:

nvidia-smi -l 5 \> gpu\_log.txt

nvidia-smi -l 5 > gpu\_log.txt

nvidia-smi -l 5 > gpu\_log.txt

Здесь **\-l 5** означает, что информация будет обновляться каждые 5 секунд и записываться в `gpu_log.txt`.

Для анализа собранных данных можно использовать `grep`, `awk` или другие утилиты. Например, чтобы получить список зафиксированных значений энергопотребления:

grep "power.draw" gpu\_log.txt

grep "power.draw" gpu\_log.txt

grep "power.draw" gpu\_log.txt

---

## Вывод

Утилита **nvidia-smi** — мощный инструмент для мониторинга и управления GPU NVIDIA. Она позволяет:  
✅ Получать информацию о состоянии видеокарты  
✅ Контролировать энергопотребление  
✅ Отслеживать загрузку и активные процессы  
✅ Вести логирование для последующего анализа

Это незаменимый инструмент как для разработчиков **CUDA**, так и для администраторов серверов с GPU.